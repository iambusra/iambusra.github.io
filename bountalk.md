---
layout: page
title: Boğaziçi Talk References
subtitle: References and readings from my Boğaziçi University talk.
description: References and materials related to my Boğaziçi University talk.
permalink: /boun-talk
sitemap:
  priority: 0.7
---


## References

### Foundations of LLMs & Beginner-Friendly Resources

- Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., & Gomez, A. N. (2017). *Attention is all you need.* *Advances in Neural Information Processing Systems*, 30(1), 261–272.  
- Brunila, M., & LaViolette, J. (2022). *What company do words keep? Revisiting the distributional semantics of J.R. Firth & Zellig Harris.* *arXiv preprint* arXiv:2205.07750.  
- Osgood, C. E. (1952). *The nature and measurement of meaning.* *Psychological Bulletin*, 49(3), 197.  
- Osgood, C. E., & Tzeng, O. (1990). *Language, meaning, and culture: The selected papers of C.E. Osgood.* Praeger Publishers.  
- Salton, G., & Lesk, M. E. (1968). *Computer evaluation of indexing and text processing.* *Journal of the ACM (JACM)*, 15(1), 8–36.  
- Salton, G. (1975). *A theory of indexing* (Vol. 18). SIAM.  
- Bengio, Y., Courville, A., & Vincent, P. (2013). *Representation learning: A review and new perspectives.* *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 35(8), 1798–1828.  
- Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). *A neural probabilistic language model.* *Journal of Machine Learning Research*, 3(Feb), 1137–1155.  
- [Serokell: Word2Vec Explained](https://serokell.io/blog/word2vec)
- [Excellent YT Channel if you are into LLM space](https://www.youtube.com/@3blue1brown)


---

### What / How Do LLMs Learn?

- McCoy, R. T., Frank, R., & Linzen, T. (2020). *Does syntax need to grow on trees? Sources of hierarchical inductive bias in sequence-to-sequence networks.* *Transactions of the Association for Computational Linguistics*, 8, 125–140.  
- Zhu, X., McCoy, R. T., & Frank, R. (2025). *The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping.* *arXiv preprint* arXiv:2508.12482.  
- Yildirim, I., & Paul, L. A. (2024). *From task structures to world models: what do LLMs know?* *Trends in Cognitive Sciences*, 28(5), 404–415.  
- Dunn, J., & Eida, M. M. (2025). *LLMs Learn Constructions That Humans Do Not Know.* *arXiv preprint* arXiv:2508.16837.  
- Prystawski, B., & Goodman, N. (2025). *Thinking fast, slow, and everywhere in between in humans and language models.*  
- Suzgun, M., Gur, T., Bianchi, F., Ho, D. E., Icard, T., Jurafsky, D., & Zou, J. (2024). *Belief in the Machine: Investigating Epistemological Blind Spots of Language Models.* *arXiv preprint* arXiv:2410.21195.

---

### Miscellaneous Articles

- Souly, A., Rando, J., Chapman, E., Davies, X., Hasircioglu, B., Shereen, E., ... & Kirk, R. (2025). *Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples.* *arXiv preprint* arXiv:2510.07192.

---

### OpenAI vs. New York Times Lawsuit

- [NYT v. OpenAI: The Times’s About-Face — *Harvard Law Review Blog* (2024)](https://harvardlawreview.org/blog/2024/04/nyt-v-openai-the-timess-about-face/)

---

### IBM Reference

- [Archived Post: “Chosting My Reply Be” (2025)](https://web.archive.org/web/20250107085756/https://cohost.org/a-hungry-mouth/post/4039145-chosting-my-reply-be)

---

### LLM Conversations

- [Conversation 1](https://chatgpt.com/share/68f1e596-1448-8003-89c3-faa9685f6975)  
- [Conversation 2](https://chatgpt.com/share/6d7e1f4e-c3d2-417b-a90c-6e081be456db)  
- [Conversation 3](https://chatgpt.com/share/68f1fb75-c108-8003-af04-f8f87c11df6b)
